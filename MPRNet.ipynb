{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MPRNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNzJxEM9DFCJ1w4Q+qUug66"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ee8d2323402a419e94340b3f77406217":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8df93f42395c4c71895bf3ff2de7eab3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fea51828162c4127b725ccd35e1d8341","IPY_MODEL_7bbe79681dbd4d3aaa1822f2eb60322d","IPY_MODEL_abb7c34f6e364cddb2bb6ef21937145f"]}},"8df93f42395c4c71895bf3ff2de7eab3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fea51828162c4127b725ccd35e1d8341":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ceee8241ba7c4cc097168602a0c9a7fc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_764cede1d40c4f97a2798badcd671197"}},"7bbe79681dbd4d3aaa1822f2eb60322d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_044935de1e4747e0adb8e7815f94e5b0","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":50,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":50,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6d4e471e9ed24e649cf349e59b266f3f"}},"abb7c34f6e364cddb2bb6ef21937145f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6490e023924f43e8b7fdd3f3f5c1912f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 50/50 [04:16&lt;00:00,  4.90s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_faf356da1bda4e0a99760cd94c6ac20c"}},"ceee8241ba7c4cc097168602a0c9a7fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"764cede1d40c4f97a2798badcd671197":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"044935de1e4747e0adb8e7815f94e5b0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6d4e471e9ed24e649cf349e59b266f3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6490e023924f43e8b7fdd3f3f5c1912f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"faf356da1bda4e0a99760cd94c6ac20c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"fvymBuobB1_i"},"source":["<a href=\"https://colab.research.google.com/github/pierclgr/SuperResolution/blob/master/MPRNet.ipynb\">\n","      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","# Preliminary steps\n","We first import all the libraries that we need."]},{"cell_type":"code","metadata":{"id":"P0veNPTdGq-b"},"source":["from builtins import tuple\n","import torch.utils.data as data\n","import torch\n","from PIL import Image\n","import torchvision.transforms as T\n","from pathlib import Path\n","import os\n","import numpy as np\n","import random\n","from tqdm.auto import tqdm\n","import h5py\n","import time\n","import zipfile"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XWfxKNYMOHVt"},"source":["Then, we set up a function to get the backend device on which the training will be executed."]},{"cell_type":"code","metadata":{"id":"aieEsNJEON0u"},"source":["# import torch_xla library if runtime is using a Colab TPU\n","if 'COLAB_TPU_ADDR' in os.environ:\n","    import torch_xla.core.xla_model as xm\n","\n","\n","def get_device() -> str:\n","    \"\"\"\n","    Get the current machine device to use\n","\n","    :returns: the current machine device\n","    \"\"\"\n","\n","    # if the current runtime is using a Colab TPU, define a flag specifying that TPU will be used\n","    if 'COLAB_TPU_ADDR' in os.environ:\n","        use_tpu = True\n","    else:\n","        use_tpu = False\n","\n","    # if TPU is available, use it as device\n","    if use_tpu:\n","        device = xm.xla_device()\n","    else:\n","        # otherwise use CUDA device or CPU accordingly to the one available\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    print(f\"\\n>>> Using {device} device\")\n","\n","    return device"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QI-SsE1wAVSM"},"source":["# Data preparation\n","\n","The first steps consists into preparing the training and testing data in order to create a PyTorch DataLoader."]},{"cell_type":"markdown","metadata":{"id":"6IyVXMaqE6TE"},"source":["## Copy dataset from Google Drive\n","As first step, we mount Google Drive in order to access the dataset zip file and then we copy it to the local Colab folder."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oWR6osgVz1mH","executionInfo":{"status":"ok","timestamp":1637506472774,"user_tz":-60,"elapsed":320898,"user":{"displayName":"Pierpasquale Colagrande","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBtuRhf9ROCXFdq91UqfNtrBBo_ScM834kCQLLrw=s64","userId":"13718066565975405160"}},"outputId":"a2ee09db-8aac-4db6-ab46-0274e623c767"},"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# copy div2k zip file\n","!echo \"Copying the dataset .zip file from Google Drive (may take some time)...\"\n","!mkdir -p /content/data/ && cp /content/drive/MyDrive/Colab\\ Notebooks/ML4CV/div2k.zip /content/data/\n","!echo \"Done!\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Copying the dataset .zip file from Google Drive (may take some time)...\n","Done!\n"]}]},{"cell_type":"markdown","metadata":{"id":"chyObWURFUdf"},"source":["## Creating HDF5 files\n","In order to load data faster, we cannot work directly on the image folders and open image files each time we create minibatch, thus we have to create HDF5 files in order to load images into minibatches faster. We first create a class representing a Div2k PyTorch dataset that loads images from HDF5 files."]},{"cell_type":"code","metadata":{"id":"6G_dn6580nmu"},"source":["class DIV2KPatchesDatasetH5(data.Dataset):\n","    \"\"\"\n","    PyTorch dataset loading DIV2K HR and LR images from a h5 file\n","    \"\"\"\n","\n","    def __init__(self, zip_file_path: str, image_format: str = \"png\", scales: list = None, split: str = \"train\",\n","                 degradation: str = \"bicubic\", patch_size: int = 64, augment: bool = True) -> None:\n","        \"\"\"\n","        Constructor method of the class\n","\n","        :param zip_file_path: path of the dataset zip file (str)\n","        :param image_format: format of the images of the dataset (str, default \"png\")\n","        :param scales: list containing the resolution scales to consider (list, default None)\n","        :param split: split of the dataset to use (str, default \"train\")\n","        :param degradation: type of degraded images to use (str, default \"bicubic\")\n","        :param patch_size: size of the square (patch_size x patch_size) lr patches to extract (int, default 64)\n","        :param augment: flag to control the augmentation of images (bool, default true)\n","        \"\"\"\n","\n","        super(DIV2KPatchesDatasetH5, self).__init__()\n","\n","        # define scales to use if not given\n","        if not scales:\n","            self.scales = [2, 3, 4]\n","        else:\n","            self.scales = scales\n","\n","        # create a list containing the string version of the given s ales\n","        scales_check = [f\"x{scale}\" for scale in self.scales]\n","\n","        # define degradation method to use\n","        self.degradation = degradation.lower()\n","\n","        # define patch size\n","        self.patch_size = patch_size\n","\n","        # define split\n","        self.split = split.lower()\n","\n","        # define to tensor function\n","        self.to_tensor = T.ToTensor()\n","\n","        # define augmentation\n","        self.augment = augment\n","\n","        # lower the image format\n","        image_format = image_format.lower()\n","\n","        # get path of the directory containing the zip file\n","        parent_path = os.path.dirname(os.path.abspath(zip_file_path))\n","\n","        # get zip file name\n","        zip_file_name = os.path.splitext(os.path.basename(os.path.abspath(zip_file_path)))[0]\n","\n","        # define the path of the folder that contains the hdf5 files\n","        hdf5_folder = os.path.join(parent_path, zip_file_name)\n","\n","        # if this folder does not exists folder does not exist\n","        if not os.path.exists(os.path.join(parent_path, zip_file_name)):\n","            # there is no hdf5 file in the folder, so they all need to be created\n","            filenames = []\n","        else:\n","            # extract files already created from the hdf5 folder\n","            _, _, filenames = next(os.walk(hdf5_folder))\n","\n","        # if file of HR images is missing for the chosen split, create it\n","        if not any('_hr' in filename and split in filename for filename in filenames):\n","            create_div2k_h5_file(zip_file_path, split=self.split, quality=\"hr\", image_format=image_format)\n","\n","        # for each scale in the given list of scales to use\n","        for scale in scales_check:\n","            # if the file of the current scale is not contained in the folder, create it\n","            if not any(split in filename and scale in filename and self.degradation in filename for filename in\n","                       filenames):\n","                create_div2k_h5_file(zip_file_path, split=self.split, quality=\"lr\", degradation=self.degradation,\n","                                     scale=scale, image_format=image_format)\n","\n","        # set the path of the folder containing the hdf5\n","        self.dataset_path = Path(hdf5_folder)\n","\n","        # open the HR images hdf5 file\n","        self.hr_file = h5py.File(os.path.join(self.dataset_path, f\"{self.split}_hr.hdf5\"), 'r')\n","\n","        # open the LR images hdf5 files\n","        self.lr_files = {\n","            scale: h5py.File(os.path.join(self.dataset_path, f\"{self.split}_lr_{self.degradation}_x{scale}.hdf5\"), 'r')\n","            for scale in self.scales}\n","\n","        # check if all the files contain the same number of images for integrity check\n","        if any(len(file.keys()) != len(self.hr_file.keys()) for _, file in self.lr_files.items()):\n","            raise Exception(\"HR and LR files does not contain the same number of images.\")\n","\n","        # define the list containing the names of the images\n","        self.file_list = list(self.hr_file.keys())\n","\n","    def __len__(self) -> int:\n","        \"\"\"\n","        Returns the length of the dataset\n","\n","        :return: length of the dataset\n","        \"\"\"\n","\n","        return len(self.file_list)\n","\n","    def __getitem__(self, item) -> tuple:\n","        \"\"\"\n","        Get a HR image and the corresponding LR images in all the scales\n","\n","        :param item: the chosen item index in the dataset\n","        \"\"\"\n","\n","        # select the image to pick\n","        file = self.file_list[item]\n","\n","        # extract the HR image\n","        hr = np.asarray(self.hr_file[file])\n","\n","        # define the output tuple as empty\n","        output_tuple = ()\n","\n","        # extract the LR images for each required scale\n","        for scale, lr_file in self.lr_files.items():\n","            # convert image to PyTorch tensor\n","            lr = np.asarray(lr_file[f'{file}x{scale}'])\n","\n","            # extract the LR and HR patches from the current scaled LR image and the HR image\n","            lr_patch, hr_patch = random_crop(lr, hr, scale)\n","\n","            # if augmentation is required\n","            if self.augment:\n","                # flip the patches\n","                lr_patch, hr_patch = random_horizontal_flip(lr_patch, hr_patch)\n","\n","                # rotate the patches\n","                lr_patch, hr_patch = random_90_rotation(lr_patch, hr_patch)\n","\n","            # add the current scale_factor-LR-HR triple to the output tuple\n","            output_tuple += (scale, self.to_tensor(lr_patch), self.to_tensor(hr_patch))\n","\n","        return output_tuple"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l5YNsFjxHaEg"},"source":["Then, we create a function that, given a folder containing images, creates the corresponding .hdf5 file."]},{"cell_type":"code","metadata":{"id":"avX5xGBaHUlV"},"source":["def create_div2k_h5_file(zip_file_path: str, split: str = \"train\", degradation: str = \"bicubic\",\n","                         scale: str = \"x2\", quality: str = \"lr\", image_format: str = \"png\") -> None:\n","    \"\"\"\n","    Create a hdf5 file containing the images of the given dataset in the required format\n","\n","    :param zip_file_path: path of the dataset zip file (str)\n","    :param split: split of the dataset to consider (str, default \"train\")\n","    :param degradation: specifies the degraded LR type of images to use (str, default \"bicubic\")\n","    :param scale: scale of the LR images to use (str, default x2)\n","    :param image_format: format of the image files (str, default \"png\")\n","    \"\"\"\n","\n","    if quality.lower() == \"lr\":\n","        print(f\"\\nCreating {quality} {degradation} {scale} {split} hdf5 file (it may take a while)...\")\n","    else:\n","        print(f\"\\nCreating {quality} {split} hdf5 file (it may take a while)...\")\n","\n","    # open the zip file\n","    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n","\n","        # get path of the directory of the zip file\n","        parent_path = os.path.dirname(os.path.abspath(zip_file.filename))\n","\n","        # get zip file name\n","        zip_file_name = os.path.splitext(os.path.basename(os.path.abspath(zip_file.filename)))[0]\n","\n","        # if hdf5 directory does not exists, create it\n","        if not os.path.exists(os.path.join(parent_path, zip_file_name)):\n","            os.makedirs(os.path.join(parent_path, zip_file_name))\n","            print(\"Hdf5 directory created.\")\n","\n","        # get all the files and folders in the zip\n","        files_and_folders = zip_file.namelist()\n","\n","        # filter the file list using the given parameters to extract the files to use for the creation\n","        extracted_files = [file for file in files_and_folders if\n","                           split.lower() in file and\n","                           quality.lower() in file and\n","                           f\".{image_format.lower()}\" in file]\n","\n","        # if the required quality is LR, filter the files also using degradation and scale\n","        if quality.lower() == \"lr\":\n","            extracted_files = [file for file in extracted_files if\n","                               degradation.lower() in file and\n","                               scale.lower() in file]\n","\n","        # sort extracted files by name in alphabetic order\n","        extracted_files.sort()\n","\n","        # define the hdf5 output file name based on the required parameters\n","        if quality.lower() == \"lr\":\n","            output_file_name = f\"{split}_{quality}_{degradation}_{scale}.hdf5\"\n","        else:\n","            output_file_name = f\"{split}_{quality}.hdf5\"\n","\n","        # open the output hdf5 file\n","        with h5py.File(os.path.join(os.path.join(parent_path, zip_file_name, output_file_name)),\n","                       \"w\") as output_file:\n","\n","            # for each image file extracted from the dataset\n","            for file in tqdm(extracted_files, total=len(extracted_files)):\n","                # open the image as a PIL image\n","                img_file = zip_file.open(file)\n","                image = Image.open(img_file)\n","\n","                # convert image to numpy array\n","                img = np.asarray(image)\n","\n","                # close PIL image and delete to free space\n","                image.close()\n","                del image\n","\n","                # extract the name of the current image file from the path\n","                image_file_name = os.path.splitext(os.path.basename(file))[0].lower()\n","\n","                # save the image on the hdf5 file\n","                output_file.create_dataset(image_file_name, data=img)\n","\n","    print(\"Done!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aRHJq55MHrJn"},"source":["We then proceed by creating the actual dataset using the hdf5 files, if present, otherwise we first create the hdf5 files by following the structure of the dataset .zip file and then we create the dataset."]},{"cell_type":"code","metadata":{"id":"rE9B7NyfH2Sr"},"source":["# create a div2k h5 dataset using the training split with bicubic degradation\n","train_ds_bicubic = DIV2KPatchesDatasetH5(\"data/div2k.zip\", split=\"train\")\n","\n","# create a div2k h5 dataset using the training split with bicubic degradation\n","val_ds_bicubic = DIV2KPatchesDatasetH5(\"data/div2k.zip\", split=\"val\")\n","\n","# create a div2k h5 dataset using the training split with unknown degradation\n","train_ds_unknown = DIV2KPatchesDatasetH5(\"data/div2k.zip\", split=\"train\", degradation=\"unknown\")\n","\n","# create a div2k h5 dataset using the training split with unknown degradation\n","val_ds_unknown = DIV2KPatchesDatasetH5(\"data/div2k.zip\", split=\"val\", degradation=\"unknown\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5rLCLxNBNBaT"},"source":["We then create the collate function for the dataloader."]},{"cell_type":"code","metadata":{"id":"N_Bbt7tMIbxj"},"source":["def collate_fn(batch: list) -> tuple:\n","    \"\"\"\n","    Collate function for the creation of a dataset\n","\n","    :param batch: list containing the batch samples extracted from the dataset using its __getitem__ method (list)\n","    :return: tuple containing the LR and HR batches in the chosen scale\n","    \"\"\"\n","\n","    # unzip the batch\n","    unzipped = list(zip(*batch))\n","\n","    # choose a random scale from the ones given for the current batch\n","    starting_sub_index = random.randint(0, int(len(unzipped) / 3) - 1) * 3\n","    scale = unzipped[starting_sub_index][0]\n","\n","    # stack the hr and lr batches into a unique PyTorch tensor\n","    lr = torch.stack(unzipped[starting_sub_index + 1])\n","    hr = torch.stack(unzipped[starting_sub_index + 2])\n","\n","    # return the batch\n","    return scale, lr, hr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DIlL6ECeNLIw"},"source":["We proceed by creating, now, the functions that augment the dataset patches by randomly rotating, flipping and cropping the patches."]},{"cell_type":"code","metadata":{"id":"K6Am8APrNKq6"},"source":["def random_crop(lr: np.ndarray, hr: np.ndarray, scale: int = 2, patch_size: int = 64) -> tuple:\n","    \"\"\"\n","    Extracts a random patch of the given size from an input lr image and the corresponding hr patch scaled using the\n","    given scale.\n","\n","    :param lr: low-resolution image to extract the patch from (ndarray)\n","    :param hr: high-resolution image to extract the patch from (ndarray)\n","    :param scale: scale to use for the extraction of the hr patch (int, default 2)\n","    :param patch_size: size of the low resolution (square) patch (int, default 64)\n","    :return: tuple containing the extacted lr and hr patches\n","    \"\"\"\n","\n","    # extract size of the lr image\n","    height, width = lr.shape[:-1]\n","\n","    # extract random starting coordinates of the patch in the lr image\n","    x = random.randint(0, width - patch_size)\n","    y = random.randint(0, height - patch_size)\n","\n","    # compute the starting coordinates of the patch in the hr image\n","    hr_patch_size = patch_size * scale\n","    hx, hy = x * scale, y * scale\n","\n","    # extract the patch from the two images\n","    lr = lr[y:y + patch_size, x:x + patch_size].copy()\n","    hr = hr[hy:hy + hr_patch_size, hx:hx + hr_patch_size].copy()\n","\n","    return lr, hr\n","\n","\n","def random_horizontal_flip(lr: np.ndarray, hr: np.ndarray, p: float = .5) -> tuple:\n","    \"\"\"\n","    Randomly applies horizontal flip to the given lr and hr patches with the given flipping probability\n","\n","    :param lr: low-resolution patch to flip (ndarray)\n","    :param hr: high-resolution patch to flip (ndarray)\n","    :param p: probability of the flipping (float, default 0.5)\n","    :return: tuple containing the flipped (or not) lr and hr patches\n","    \"\"\"\n","\n","    # flip horizontally the images\n","    if random.random() < p:\n","        lr = np.fliplr(lr)\n","        hr = np.fliplr(hr)\n","\n","    return lr.copy(), hr.copy()\n","\n","\n","def random_90_rotation(lr: np.ndarray, hr: np.ndarray) -> tuple:\n","    \"\"\"\n","    Randomly applies a 90° rotation (or not) to the given lr and hr patches\n","\n","    :param lr: low-resolution patch to rotate (ndarray)\n","    :param hr: high-resolution patch to rotate (ndarray)\n","    :return: tuple containing the rotated (or not) lr and hr patches\n","    \"\"\"\n","\n","    # choose a rotation angle (0, 90, -90)\n","    n_rotations = random.choice([0, 1, 3])\n","\n","    # rotate the images\n","    lr = np.rot90(lr, n_rotations)\n","    hr = np.rot90(hr, n_rotations)\n","\n","    return lr.copy(), hr.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fyBZ3YFQNZAW"},"source":["We now create the dataloader for div2k dataset and we test its loading speed."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["ee8d2323402a419e94340b3f77406217","8df93f42395c4c71895bf3ff2de7eab3","fea51828162c4127b725ccd35e1d8341","7bbe79681dbd4d3aaa1822f2eb60322d","abb7c34f6e364cddb2bb6ef21937145f","ceee8241ba7c4cc097168602a0c9a7fc","764cede1d40c4f97a2798badcd671197","044935de1e4747e0adb8e7815f94e5b0","6d4e471e9ed24e649cf349e59b266f3f","6490e023924f43e8b7fdd3f3f5c1912f","faf356da1bda4e0a99760cd94c6ac20c"]},"id":"X6hQ25pvNVII","executionInfo":{"status":"ok","timestamp":1637507662980,"user_tz":-60,"elapsed":256881,"user":{"displayName":"Pierpasquale Colagrande","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBtuRhf9ROCXFdq91UqfNtrBBo_ScM834kCQLLrw=s64","userId":"13718066565975405160"}},"outputId":"1c625a6a-e89e-48d3-dab9-1c2741173c3a"},"source":["dload = data.DataLoader(train_ds_bicubic, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n","\n","start = time.time()\n","for scale, lr, hr in tqdm(dload):\n","    # print(scale, lr.size(), hr.size())\n","    pass\n","end = time.time()\n","\n","print(end - start)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee8d2323402a419e94340b3f77406217","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["256.73977971076965\n"]}]}]}